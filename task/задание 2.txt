ФП, задание 2

Написать консольную утилиту на языке Clojure, которая будет выполнять функции кроулера (web crawler).

Параметры коммандной строки утилиты - файл и глубина.

Файл содержит список урлов, с которых кроулер начинает обход. Формат файла: 1 строка - 1 урл.
Глубина определяет глубину обхода веб-графа. Если глубина = 1, кроулер обходит только те урлы, которые указаны в файле.

Для каждого адреса из файла читается страница, расположеная по этому адресу. 
Далее она парсится и считается количество ссылок, расположеных на ней. 
Если нет ответа от сервера или сервер вернул 404 страница помечается, как "плохая".
Если глубина больше 1, то тоже самое делается со страницами, ссылки на которые были расположен на уже распаршеной странице.
И так далее, до тех пор пока позволяет глубина.

На консоль выводится результат следующего формата:

url1 10 links
	url11 bad 
	url12 15 redirect url13
url2 bad

В нормальной ситуации выводится количество ссылок, расположеных на странице. 
"Bad" выводится если небыло ответа от сервера или вернулся код 404.    
"Redirect" и урл выводится если произошло перенаправление.
Урлы для сдвинутых строк вывода были взяты из ссылок на странице предыдущего уровня.

При реализации программы требуется воспользоваться хотя-бы одним из следующих объектов clojure - atoms, refs, agents.

Парсинг страниц должен происходить параллельно (т.е. 1 страница парсится 1-м потоком, но одновременно должно парсится несколько страниц).
Количество потоков должно быть ограничено.
Можно использовать потоки и пулы потоков java.
Для работы с http и парсинга html можно использовать готовые библиотеки.

Так же, необходимо написать по крайней мере 2 юнит-теста:
1-ый должен показать, что ваш код умеет парсить html и правильно подсчитывать количество ссылок на странице
2-ой должен показать, что ваш код правильно умеет обрабатывать ситуацию 404/нет ответа.
 
